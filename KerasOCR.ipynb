{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGCUT9dcyiIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8228842-5e9a-404d-83c2-31d216481c8b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-ocr\n",
            "  Downloading keras_ocr-0.9.3-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.6.2)\n",
            "Collecting efficientnet==1.0.0 (from keras-ocr)\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting essential_generators (from keras-ocr)\n",
            "  Downloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fonttools in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (4.51.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (0.4.0)\n",
            "Collecting pyclipper (from keras-ocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (2.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from keras-ocr) (4.66.4)\n",
            "Collecting validators (from keras-ocr)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet==1.0.0->keras-ocr)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet==1.0.0->keras-ocr) (0.19.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->keras-ocr) (2.31.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (3.9.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (3.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (2024.5.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (24.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->keras-ocr) (2.8.2)\n",
            "Installing collected packages: pyclipper, essential_generators, validators, keras-applications, efficientnet, keras-ocr\n",
            "Successfully installed efficientnet-1.0.0 essential_generators-1.0 keras-applications-1.0.8 keras-ocr-0.9.3 pyclipper-1.3.0.post5 validators-0.28.1\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 2s (99.8 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-ocr\n",
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines based on the bounding box coordinates.\n",
        "    Words are grouped into the same line if their bounding boxes' vertical (y)\n",
        "    midpoints are close enough, indicating they are part of the same line of text.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    last_median_y = None\n",
        "\n",
        "    for word, box in prediction_groups[0]:\n",
        "        # Calculate the median y-coordinate of the bounding box to determine the line\n",
        "        y_coords = [point[1] for point in box]\n",
        "        median_y = sum(y_coords) / len(y_coords)\n",
        "\n",
        "        # If there's a significant vertical distance to the previous word, start a new line\n",
        "        if last_median_y is not None and abs(median_y - last_median_y) > 10:  # Adjust threshold as needed\n",
        "            lines.append(\" \".join(current_line))\n",
        "            current_line = []\n",
        "\n",
        "        current_line.append(word)\n",
        "        last_median_y = median_y\n",
        "\n",
        "    # Don't forget to add the last line\n",
        "    if current_line:\n",
        "        lines.append(\" \".join(current_line))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Initialize the keras-ocr pipeline\n",
        "pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "pdf_path = '2.pdf'\n",
        "xml_file = os.path.splitext(pdf_path)[0] + '.xml'\n",
        "\n",
        "root = ET.Element(\"Document\")\n",
        "images = convert_from_path(pdf_path)\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    # Convert PIL image to a numpy array\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    # Ensure image is in RGB format if it's a grayscale image\n",
        "    if len(image_np.shape) == 2:  # Image is grayscale\n",
        "        image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "    # Perform OCR using keras-ocr\n",
        "    prediction_groups = pipeline.recognize([image_np])\n",
        "\n",
        "    lines = group_text_by_lines(prediction_groups)\n",
        "    clean_lines = [clean_text(line) for line in lines]\n",
        "\n",
        "    page_element = ET.SubElement(root, \"Page\", number=str(i+1))\n",
        "    for line_num, line in enumerate(clean_lines):\n",
        "        line_element = ET.SubElement(page_element, \"Line\", number=str(line_num+1))\n",
        "        line_element.text = line\n",
        "\n",
        "tree = ET.ElementTree(root)\n",
        "tree.write(xml_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "print(f\"The program is done, please check the directory! {xml_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8y1xzWBFqOt",
        "outputId": "63d44bf7-702d-47da-a390-0b0b05351f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Downloading /root/.keras-ocr/craft_mlt_25k.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "4/4 [==============================] - 3s 133ms/step\n",
            "The program is done, please check the directory! 2.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the CER and WER\n",
        "import os\n",
        "import keras_ocr\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def preprocess_image(image, size=(1280, 720)):\n",
        "    \"\"\"\n",
        "    Preprocess the image by resizing, converting to grayscale, and applying Gaussian blur.\n",
        "    Then convert it back to RGB as keras-ocr expects a three-channel image.\n",
        "    \"\"\"\n",
        "    image = image.resize(size)\n",
        "    gray_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    preprocessed_image = cv2.cvtColor(blurred_image, cv2.COLOR_GRAY2RGB)\n",
        "    return preprocessed_image\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text by removing invalid XML characters and other non-alphanumeric characters.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    text = invalid_xml_chars.sub('', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def process_documents(directory):\n",
        "    # Setup the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(directory, filename)\n",
        "            ground_truth_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "            if not os.path.exists(ground_truth_path):\n",
        "                print(f\"Missing ground truth text file for {filename}\")\n",
        "                continue\n",
        "\n",
        "            images = convert_from_path(pdf_path)\n",
        "            all_text = []\n",
        "\n",
        "            for image in images:\n",
        "                preprocessed_image = preprocess_image(image)\n",
        "                prediction_groups = pipeline.recognize([np.array(preprocessed_image)])\n",
        "                for predictions in prediction_groups:\n",
        "                    text = ' '.join([word[0] for word in predictions])\n",
        "                    clean_text_data = clean_text(text)\n",
        "                    all_text.append(clean_text_data)\n",
        "\n",
        "            ocr_output_text = '\\n'.join(all_text)\n",
        "\n",
        "            with open(ground_truth_path, 'r', encoding='utf-8') as gt_file:\n",
        "                ground_truth_text = gt_file.read()\n",
        "                ground_truth_text = clean_text(ground_truth_text)\n",
        "\n",
        "            calculated_wer = wer(ground_truth_text, ocr_output_text)\n",
        "            calculated_cer = cer(ground_truth_text, ocr_output_text)\n",
        "\n",
        "            print(f\"\\n{filename} - WER: {calculated_wer:.2f}\")\n",
        "            print(f\"{filename} - CER: {calculated_cer:.2f}\")\n",
        "\n",
        "directory_path = 'text/'\n",
        "process_documents(directory_path)\n"
      ],
      "metadata": {
        "id": "IwQQ7zGazddZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9796429a-08c9-4ee4-f0bb-7f7055777677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "4/4 [==============================] - 2s 15ms/step\n",
            "\n",
            "2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.pdf - WER: 1.02\n",
            "2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.pdf - CER: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting metadata based on predefined keywords\n",
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # keras-ocr doesn't provide a confidence, so we use a dummy value\n",
        "    return lines\n",
        "\n",
        "def extract_first_occurrence(lines, keywords, text_limits):\n",
        "    \"\"\"\n",
        "    Extracts the first occurrence of each keyword and ignores subsequent mentions.\n",
        "    \"\"\"\n",
        "    pairs = {}\n",
        "    current_key = None\n",
        "    current_text = []\n",
        "    current_limit = None\n",
        "    found_keywords = set()\n",
        "\n",
        "    normalized_keywords = {keyword.lower(): keyword for keyword in keywords}\n",
        "\n",
        "    for text, _, _ in lines:\n",
        "        normalized_text = text.lower().strip()\n",
        "\n",
        "        if any(keyword == normalized_text for keyword in normalized_keywords) and normalized_text not in found_keywords:\n",
        "            if current_key:\n",
        "                formatted_text = ' '.join(current_text).strip()\n",
        "                if current_limit is not None:\n",
        "                    formatted_text = formatted_text[:current_limit]\n",
        "                pairs[current_key] = formatted_text\n",
        "            current_key = normalized_keywords.get(normalized_text, text.strip())\n",
        "            current_text = []\n",
        "            current_limit = text_limits.get(current_key, None)\n",
        "            found_keywords.add(normalized_text)  # Mark this keyword as found\n",
        "        elif current_key:\n",
        "            current_text.append(text.strip())\n",
        "\n",
        "    if current_key:\n",
        "        formatted_text = ' '.join(current_text).strip()\n",
        "        if current_limit is not None:\n",
        "            formatted_text = formatted_text[:current_limit]\n",
        "        pairs[current_key] = formatted_text\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def write_to_xml(lines, xml_path):\n",
        "    root = ET.Element(\"root\")\n",
        "    for text, _, _ in lines:\n",
        "        element = ET.SubElement(root, \"line\")\n",
        "        element.text = text\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(xml_path)\n",
        "\n",
        "def read_from_xml(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    return [(line.text, None, None) for line in root]\n",
        "\n",
        "def main():\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '4.pdf'\n",
        "    xml_path = os.path.splitext(pdf_path)[0] + '.xml'\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    keywords = [\"Fastighetsagare\", \"Namn\", \"Postnr och ort\", \"Fastighetsbeteckning\", \"Anlaggare av ledning\"]\n",
        "    text_limits = {\"Anlaggare av ledning\": 41}  # For last keyword\n",
        "\n",
        "    all_lines = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "        # Perform OCR\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        all_lines.extend(lines)\n",
        "    write_to_xml(all_lines, xml_path)\n",
        "    lines_from_xml = read_from_xml(xml_path)\n",
        "    info = extract_first_occurrence(lines_from_xml, keywords, text_limits)\n",
        "\n",
        "    for key in keywords:\n",
        "        value = info.get(key, \"\")\n",
        "        print(f\"{key}: {value}\")  # Print each keyword and its first extracted value\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "051BuyMFHxDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # Assuming a dummy confidence value\n",
        "    return lines\n",
        "\n",
        "def write_to_xml(lines, xml_file):\n",
        "    \"\"\"\n",
        "    Write the extracted lines to an XML file with bounding box and confidence.\n",
        "    \"\"\"\n",
        "    root = ET.Element(\"Document\")\n",
        "    for text, bbox, confidence in lines:\n",
        "        item = ET.SubElement(root, \"Line\")\n",
        "        item.set('bbox', str(bbox))\n",
        "        item.set('confidence', str(confidence))\n",
        "        item.text = clean_text(text)\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(xml_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "def read_and_search_keywords(xml_file, keywords):\n",
        "    \"\"\"\n",
        "    Read the XML file and search for the first occurrence of each keyword.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    results = {}\n",
        "    found_keywords = set()\n",
        "\n",
        "    text_lines = [elem.text or \"\" for elem in root.findall('Line')]\n",
        "\n",
        "    # Concatenate all texts into a single content block to mimic continuous reading\n",
        "    full_text = \" \".join(text_lines).lower()\n",
        "    keyword_positions = {kw.lower(): full_text.find(kw.lower()) for kw in keywords if full_text.find(kw.lower()) != -1}\n",
        "    # Sort keywords by their first occurrence position\n",
        "    sorted_keywords = sorted(keyword_positions.items(), key=lambda x: x[1])\n",
        "\n",
        "    for i, (kw, pos) in enumerate(sorted_keywords):\n",
        "        # Find end of the text slice for this keyword\n",
        "        if i < len(sorted_keywords) - 1:\n",
        "            end_pos = sorted_keywords[i + 1][1]\n",
        "        else:\n",
        "            end_pos = len(full_text)\n",
        "        # Extract the text segment associated with the keyword\n",
        "        segment = full_text[pos:end_pos].strip()\n",
        "        results[kw] = segment\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '2.pdf'\n",
        "    xml_path = os.path.splitext(pdf_path)[0] + '.xml'\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    all_lines = []\n",
        "    for image in images:\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB if needed\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        all_lines.extend(lines)\n",
        "\n",
        "    write_to_xml(all_lines, xml_path)\n",
        "    print(f\"XML file has been created: {xml_path}\")\n",
        "\n",
        "    keywords = [\"Fastighetsagare\", \"Fastighetsbeteckning\", \"anlaggare\", \"ort, datum\", \"organisationsnumen\" \"Anlaggare av ledning\"]\n",
        "    search_results = read_and_search_keywords(xml_path, keywords)\n",
        "    for kw, content in search_results.items():\n",
        "        print(f\"{kw.capitalize()}: {content}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1Zj9fHgQJuWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate CER, WER just for one PDF\n",
        "import os\n",
        "import keras_ocr\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def preprocess_image(image, size=(1280, 720)):\n",
        "    \"\"\"\n",
        "    Preprocess the image by resizing, converting to grayscale, and applying Gaussian blur.\n",
        "    Then convert it back to RGB as keras-ocr expects a three-channel image.\n",
        "    \"\"\"\n",
        "    image = image.resize(size)\n",
        "    gray_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    preprocessed_image = cv2.cvtColor(blurred_image, cv2.COLOR_GRAY2RGB)\n",
        "    return preprocessed_image\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text by removing invalid XML characters and other non-alphanumeric characters.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    text = invalid_xml_chars.sub('', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def process_single_document(pdf_path, ground_truth_path):\n",
        "    # Setup the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    if not os.path.exists(ground_truth_path):\n",
        "        print(f\"Missing ground truth text file for {pdf_path}\")\n",
        "        return\n",
        "\n",
        "    images = convert_from_path(pdf_path)\n",
        "    all_text = []\n",
        "\n",
        "    for image in images:\n",
        "        preprocessed_image = preprocess_image(image)\n",
        "        prediction_groups = pipeline.recognize([np.array(preprocessed_image)])\n",
        "        for predictions in prediction_groups:\n",
        "            text = ' '.join([word[0] for word in predictions])\n",
        "            clean_text_data = clean_text(text)\n",
        "            all_text.append(clean_text_data)\n",
        "\n",
        "    ocr_output_text = '\\n'.join(all_text)\n",
        "\n",
        "    with open(ground_truth_path, 'r', encoding='utf-8') as gt_file:\n",
        "        ground_truth_text = gt_file.read()\n",
        "        ground_truth_text = clean_text(ground_truth_text)\n",
        "\n",
        "    calculated_wer = wer(ground_truth_text, ocr_output_text)\n",
        "    calculated_cer = cer(ground_truth_text, ocr_output_text)\n",
        "\n",
        "    print(f\"\\nOCR Text: {ocr_output_text}\")\n",
        "    print(f\"\\nGround Truth Text: {ground_truth_text}\")\n",
        "    print(f\"\\nWER: {calculated_wer:.2f}\")\n",
        "    print(f\"CER: {calculated_cer:.2f}\")\n",
        "\n",
        "\n",
        "pdf_path = '2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.pdf'\n",
        "ground_truth_path = '2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.txt'\n",
        "process_single_document(pdf_path, ground_truth_path)\n"
      ],
      "metadata": {
        "id": "QikUmuTxOmWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert pdf to txt file\n",
        "!pip install pdfminer.six\n",
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "def convert_pdf_to_txt(pdf_path, txt_path):\n",
        "    \"\"\"\n",
        "    Convert a PDF file to a text file using pdfminer.six.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = extract_text(pdf_path)\n",
        "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "            txt_file.write(text)\n",
        "        print(f\"Converted {pdf_path} to {txt_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {pdf_path}: {str(e)}\")\n",
        "\n",
        "def convert_pdfs_in_directory(directory):\n",
        "    \"\"\"\n",
        "    Convert all PDF files to text files.\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(directory, filename)\n",
        "            txt_path = os.path.join(directory, filename.replace('.pdf', '.txt'))\n",
        "            convert_pdf_to_txt(pdf_path, txt_path)\n",
        "\n",
        "def main():\n",
        "    directory_path = 'text/'\n",
        "    convert_pdfs_in_directory(directory_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQXsUP0narWi",
        "outputId": "29d53701-2b5b-4240-ced5-91e604315392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Converted text/7MHCNBM2ZR4PD2BSWVFLG77BRN3HS3AD.pdf to text/7MHCNBM2ZR4PD2BSWVFLG77BRN3HS3AD.txt\n",
            "Converted text/27UIROOYZ4IE3FKKNAPCKEQXWBKKN7XM.pdf to text/27UIROOYZ4IE3FKKNAPCKEQXWBKKN7XM.txt\n",
            "Converted text/7LAHAWE6SGPL25FWUGMRKXCMTD3CWEBO.pdf to text/7LAHAWE6SGPL25FWUGMRKXCMTD3CWEBO.txt\n",
            "Converted text/7CIFOV7XZXS6EHUFYZF6H3NCN637PI2M.pdf to text/7CIFOV7XZXS6EHUFYZF6H3NCN637PI2M.txt\n",
            "Converted text/7GYQLFBVGRTYWE73NKCNCODTJVURFULB.pdf to text/7GYQLFBVGRTYWE73NKCNCODTJVURFULB.txt\n",
            "Converted text/73KIZM2KPVDJHMKWM5X36DBUKAOOCWZ4.pdf to text/73KIZM2KPVDJHMKWM5X36DBUKAOOCWZ4.txt\n",
            "Converted text/6C5QST2YJX7GA4BOHPZK66SWOSEATYGO.pdf to text/6C5QST2YJX7GA4BOHPZK66SWOSEATYGO.txt\n",
            "Converted text/6X3XULQL4IDEVAAER5F2M3RJGJS2REAH.pdf to text/6X3XULQL4IDEVAAER5F2M3RJGJS2REAH.txt\n",
            "Converted text/7PZU4ANBX7CFDAKIK25A2U6GR5OMGEZX.pdf to text/7PZU4ANBX7CFDAKIK25A2U6GR5OMGEZX.txt\n",
            "Converted text/35DKLURQRIKJHCGH3IP5TO2BTAOV4CBN.pdf to text/35DKLURQRIKJHCGH3IP5TO2BTAOV4CBN.txt\n",
            "Converted text/6GHFGNIZJZWEVRNG4J5ILE62VJ3PUURO.pdf to text/6GHFGNIZJZWEVRNG4J5ILE62VJ3PUURO.txt\n",
            "Converted text/6RZXET5EGN7QMGM5PROIXJI447YTCETZ.pdf to text/6RZXET5EGN7QMGM5PROIXJI447YTCETZ.txt\n",
            "Converted text/32PQ6TLRU26TDGA4NLYBMYEHYGX3X7US.pdf to text/32PQ6TLRU26TDGA4NLYBMYEHYGX3X7US.txt\n",
            "Converted text/6MPUNFSLSFKNLOGE2I6WRELWSJOF27UK.pdf to text/6MPUNFSLSFKNLOGE2I6WRELWSJOF27UK.txt\n",
            "Converted text/7YLE2QQTJK2HP5N2PUSAA3IKH662U3Y6.pdf to text/7YLE2QQTJK2HP5N2PUSAA3IKH662U3Y6.txt\n",
            "Converted text/7IYBO6EFOFMF4ETCCIZYJEGPTBQY3EE5.pdf to text/7IYBO6EFOFMF4ETCCIZYJEGPTBQY3EE5.txt\n",
            "Converted text/6Z3CUWVPRPVEBPROWQCD6EZJ22CLL5OY.pdf to text/6Z3CUWVPRPVEBPROWQCD6EZJ22CLL5OY.txt\n",
            "Converted text/7HBBXA2H7AQGVZSBZTLTPKAAEE3ZY2YK.pdf to text/7HBBXA2H7AQGVZSBZTLTPKAAEE3ZY2YK.txt\n",
            "Converted text/6OV4KF7OC77SI4QQ6W5MKPRKLFKJFMSA.pdf to text/6OV4KF7OC77SI4QQ6W5MKPRKLFKJFMSA.txt\n",
            "Converted text/35CQZCOMND26DXY46FVVS47WB2AL75OK.pdf to text/35CQZCOMND26DXY46FVVS47WB2AL75OK.txt\n",
            "Converted text/7RVVK7WHY6HBTX3RU6AVLEEPLNLNQQDZ.pdf to text/7RVVK7WHY6HBTX3RU6AVLEEPLNLNQQDZ.txt\n",
            "Converted text/6LM5MPDEKTK4FAVUBSU5HZOLO33XZOV6.pdf to text/6LM5MPDEKTK4FAVUBSU5HZOLO33XZOV6.txt\n",
            "Converted text/6AVXT3OLPGYVAG7L4CBYJHXLAXJE4V6F.pdf to text/6AVXT3OLPGYVAG7L4CBYJHXLAXJE4V6F.txt\n",
            "Converted text/7N6KRBZIEFV4F5QLLW3GBF6LKNNWSWVB.pdf to text/7N6KRBZIEFV4F5QLLW3GBF6LKNNWSWVB.txt\n",
            "Converted text/7DHC66NNE4RHBN7MNVPKSCN6E2GFRS2Z.pdf to text/7DHC66NNE4RHBN7MNVPKSCN6E2GFRS2Z.txt\n",
            "Converted text/7A3MBRLFC6OU5KGMFIDEQPUOQTROBYUS.pdf to text/7A3MBRLFC6OU5KGMFIDEQPUOQTROBYUS.txt\n",
            "Converted text/6A2V6MYYSW4C6QUKXJX7BEGNPYKZGV6O.pdf to text/6A2V6MYYSW4C6QUKXJX7BEGNPYKZGV6O.txt\n",
            "Converted text/6X7DGVAM2PXFTZ3B6YQOHRCZ3VVBS5LO.pdf to text/6X7DGVAM2PXFTZ3B6YQOHRCZ3VVBS5LO.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the time duration\n",
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # keras-ocr doesn't provide a confidence, so we use a dummy value\n",
        "    return lines\n",
        "\n",
        "def extract_specific_info_from_lines(lines, keywords, text_limits):\n",
        "    \"\"\"\n",
        "    Extracts specific information based on predefined keywords.\n",
        "    \"\"\"\n",
        "    pairs = {}\n",
        "    current_key = None\n",
        "    current_text = []\n",
        "    current_limit = None\n",
        "    normalized_keywords = {keyword.lower(): keyword for keyword in keywords}\n",
        "\n",
        "    for text, _, _ in lines:\n",
        "        normalized_text = text.lower().strip()\n",
        "        if any(normalized_text.startswith(keyword) for keyword in normalized_keywords):\n",
        "            if current_key:\n",
        "                formatted_text = ' '.join(current_text).strip()\n",
        "                if current_limit is not None:\n",
        "                    formatted_text = formatted_text[:current_limit]\n",
        "                pairs[current_key] = formatted_text\n",
        "            current_key = normalized_keywords.get(normalized_text, text.strip())\n",
        "            current_text = []\n",
        "            current_limit = text_limits.get(current_key, None)\n",
        "        elif current_key:\n",
        "            current_text.append(text.strip())\n",
        "\n",
        "    if current_key:\n",
        "        formatted_text = ' '.join(current_text).strip()\n",
        "        if current_limit is not None:\n",
        "            formatted_text = formatted_text[:current_limit]\n",
        "        pairs[current_key] = formatted_text\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '4.pdf'\n",
        "    images = convert_from_path(pdf_path)\n",
        "    keywords = [\"Fastighetsagare\", \"Namn\", \"Postnr och ort\", \"Fastighetsbeteckning\", \"Anlaggare av ledning\"]\n",
        "    text_limits = {\"Anlaggare av ledning\": 41}\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        image_processing_start = time.time()  # Start timing the processing for each image\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "        # Perform OCR\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        info = extract_specific_info_from_lines(lines, keywords, text_limits)\n",
        "\n",
        "        for key in keywords:\n",
        "            value = info.get(key, \"\")\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "        image_processing_end = time.time()\n",
        "        print(f\"Processed image {i+1} in {image_processing_end - image_processing_start} seconds\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total OCR processing time: {end_time - start_time} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "rmQg_cwpA46L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}