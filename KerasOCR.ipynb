{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGCUT9dcyiIm"
      },
      "outputs": [],
      "source": [
        "!pip install keras-ocr\n",
        "!pip install pdf2image\n",
        "!apt-get install poppler-utils\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the CER and WER\n",
        "import os\n",
        "import keras_ocr\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def preprocess_image(image, size=(1280, 720)):\n",
        "    \"\"\"\n",
        "    Preprocess the image by resizing, converting to grayscale, and applying Gaussian blur.\n",
        "    Then convert it back to RGB as keras-ocr expects a three-channel image.\n",
        "    \"\"\"\n",
        "    image = image.resize(size)\n",
        "    gray_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    preprocessed_image = cv2.cvtColor(blurred_image, cv2.COLOR_GRAY2RGB)\n",
        "    return preprocessed_image\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text by removing invalid XML characters and other non-alphanumeric characters.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    text = invalid_xml_chars.sub('', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def process_documents(directory):\n",
        "    # Setup the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(directory, filename)\n",
        "            ground_truth_path = pdf_path.replace('.pdf', '.txt')\n",
        "\n",
        "            if not os.path.exists(ground_truth_path):\n",
        "                print(f\"Missing ground truth text file for {filename}\")\n",
        "                continue\n",
        "\n",
        "            images = convert_from_path(pdf_path)\n",
        "            all_text = []\n",
        "\n",
        "            for image in images:\n",
        "                preprocessed_image = preprocess_image(image)\n",
        "                prediction_groups = pipeline.recognize([np.array(preprocessed_image)])\n",
        "                for predictions in prediction_groups:\n",
        "                    text = ' '.join([word[0] for word in predictions])\n",
        "                    clean_text_data = clean_text(text)\n",
        "                    all_text.append(clean_text_data)\n",
        "\n",
        "            ocr_output_text = '\\n'.join(all_text)\n",
        "\n",
        "            with open(ground_truth_path, 'r', encoding='utf-8') as gt_file:\n",
        "                ground_truth_text = gt_file.read()\n",
        "                ground_truth_text = clean_text(ground_truth_text)\n",
        "\n",
        "            calculated_wer = wer(ground_truth_text, ocr_output_text)\n",
        "            calculated_cer = cer(ground_truth_text, ocr_output_text)\n",
        "\n",
        "            print(f\"\\n{filename} - WER: {calculated_wer:.2f}\")\n",
        "            print(f\"{filename} - CER: {calculated_cer:.2f}\")\n",
        "\n",
        "directory_path = 'large/'\n",
        "process_documents(directory_path)\n"
      ],
      "metadata": {
        "id": "IwQQ7zGazddZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting metadata based on predefined keywords\n",
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # keras-ocr doesn't provide a confidence, so we use a dummy value\n",
        "    return lines\n",
        "\n",
        "def extract_first_occurrence(lines, keywords, text_limits):\n",
        "    \"\"\"\n",
        "    Extracts the first occurrence of each keyword and ignores subsequent mentions.\n",
        "    \"\"\"\n",
        "    pairs = {}\n",
        "    current_key = None\n",
        "    current_text = []\n",
        "    current_limit = None\n",
        "    found_keywords = set()\n",
        "\n",
        "    normalized_keywords = {keyword.lower(): keyword for keyword in keywords}\n",
        "\n",
        "    for text, _, _ in lines:\n",
        "        normalized_text = text.lower().strip()\n",
        "\n",
        "        if any(keyword == normalized_text for keyword in normalized_keywords) and normalized_text not in found_keywords:\n",
        "            if current_key:\n",
        "                formatted_text = ' '.join(current_text).strip()\n",
        "                if current_limit is not None:\n",
        "                    formatted_text = formatted_text[:current_limit]\n",
        "                pairs[current_key] = formatted_text\n",
        "            current_key = normalized_keywords.get(normalized_text, text.strip())\n",
        "            current_text = []\n",
        "            current_limit = text_limits.get(current_key, None)\n",
        "            found_keywords.add(normalized_text)  # Mark this keyword as found\n",
        "        elif current_key:\n",
        "            current_text.append(text.strip())\n",
        "\n",
        "    if current_key:\n",
        "        formatted_text = ' '.join(current_text).strip()\n",
        "        if current_limit is not None:\n",
        "            formatted_text = formatted_text[:current_limit]\n",
        "        pairs[current_key] = formatted_text\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def write_to_xml(lines, xml_path):\n",
        "    root = ET.Element(\"root\")\n",
        "    for text, _, _ in lines:\n",
        "        element = ET.SubElement(root, \"line\")\n",
        "        element.text = text\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(xml_path)\n",
        "\n",
        "def read_from_xml(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    return [(line.text, None, None) for line in root]\n",
        "\n",
        "def main():\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '4.pdf'\n",
        "    xml_path = os.path.splitext(pdf_path)[0] + '.xml'\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    keywords = [\"Fastighetsagare\", \"Namn\", \"Postnr och ort\", \"Fastighetsbeteckning\", \"Anlaggare av ledning\"]\n",
        "    text_limits = {\"Anlaggare av ledning\": 41}  # For last keyword\n",
        "\n",
        "    all_lines = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "        # Perform OCR\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        all_lines.extend(lines)\n",
        "\n",
        "    # Write OCR results to XML\n",
        "    write_to_xml(all_lines, xml_path)\n",
        "\n",
        "    # Read from XML\n",
        "    lines_from_xml = read_from_xml(xml_path)\n",
        "    info = extract_first_occurrence(lines_from_xml, keywords, text_limits)\n",
        "\n",
        "    for key in keywords:\n",
        "        value = info.get(key, \"\")\n",
        "        print(f\"{key}: {value}\")  # Print each keyword and its first extracted value\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "051BuyMFHxDc",
        "outputId": "8e2e8d83-d7cb-44fb-9671-f192d7dc1ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 0s 486ms/step\n",
            "9/9 [==============================] - 2s 17ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "7/7 [==============================] - 0s 15ms/step\n",
            "Fastighetsagare: \n",
            "Namn: l arabere f adress nals so l s postnr och ort 412 nonsto\n",
            "Postnr och ort: \n",
            "Fastighetsbeteckning: anlaggare ledning norsjo kommun av storgatan 67 norsjo 935 81 2120002858 organisationsnr med underjordisk ledning for data och telekommunikation enligt detta avtal alla avses kablar och ledningar vilka overfors signaler for bild data eller i t genom ex annat underjordisk ledning ingar sadana andamal erforderliga for dess anordningar brunnar som skarvlador tillbehor tomror skap och andra fastighetsagaren till pa fastighet for anlaggaren ratt markutrymme angiven anl aggning ger bibehallande underhall ande underjordiska for och nyttji ledningar data och av telekommunikation disponera det behovs utfora for arbete enligt den samt utrymme att som strackning redovisas pa bifogad karta som vid byte anlaggningsagare overgar anderatten automatiskt till den nytti av nye anlaggningsagaren anlaggningsarbetet paborjas under tiden 201710515 20171030 ska placering kabelskap och andra anordningar mark eventuella kabelbrunnar samt ska av ovan godkannas fastighetsagaren av skyndsamt aterstalla hade anlaggaren ska till det skick den innan respektive arbete marken paborjades tillfartsvagar hallas farbara ska sa langt det ar mojligt under arbetets gang fastighetsagaren stamkabelgrav ombesorjer gravning fran och in till den fastigheten egna fastighetsagaren pa delen det aligger aterstalla marken fran stamkabelgrav till att egen fastighet till det den hade arbetet paborjades tillfartsvagar sa langt skick innan ska det ar mojligt hallas farbara under arbetets gang nagon ersattning inte utga fastighetsagaren anledning skall mellan anl aggaren och med ay detta avtal tillhorig for skada pa fast egendom fastighetsagaren och orsakats anlaggaren ska som av traffas uppgorelse ersattning 1 varje enskilt fall om norsio lansstyrelsen kommun vasterbotten europelska urdorueonden tor anostyosutecing europa landsbyodsoma den esicrar markomrade eller pa fastighetsagaren forbinder inom upplatet inte bygga satt sig att annat forbinder vid tillfalle forsvara atkomsten kanalisationen fastighetsagaren sig varje att av underhalla ledningarna medverka till mojligheter underl attas att anlaggarens att om ghetsagaren overlater fastighet byggnad eller anl aggning beror fastis annan som enligt forhallande till forvarvaren anlaggarens detta avtal ska fastighetsagaren i gora ratt forbehall for enligt detta anlaggarens ratt avtal vid forsalining eller overlatelse fastighet ska kontakt med norsjo kommun for tas annan av upprattande kontrakt nytt av eventuell flyttning under gallande underjordisk ledning bekostas den avtal part av som av ha atgarden detta galler saval under gallande avtalet avtal nar sagts anses nytta som av upp manader fore avtalstidens avtalet giltigt i 25 ar om avtalet inte 6 ar sags senast upp utgang forlangs det tills vidare med s ar 1 taget fastighetsagaren tagit exemplar detta har upprattats l tva exemplar vilka ett och avtal av anlaggaren tagit exemplar ett 20 toirs tosos arabers norsjo datum ort datum donrens ds dol icmas fastighetsagare norsjo kommun canlaggaren khrmt lanjer lunuberg mnucel lom wilnfim as for norsjo kommun namnfortydligande textal c fastighetsagare namnfortydligande\n",
            "Anlaggare av ledning: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # Assuming a dummy confidence value\n",
        "    return lines\n",
        "\n",
        "def write_to_xml(lines, xml_file):\n",
        "    \"\"\"\n",
        "    Write the extracted lines to an XML file with bounding box and confidence.\n",
        "    \"\"\"\n",
        "    root = ET.Element(\"Document\")\n",
        "    for text, bbox, confidence in lines:\n",
        "        item = ET.SubElement(root, \"Line\")\n",
        "        item.set('bbox', str(bbox))\n",
        "        item.set('confidence', str(confidence))\n",
        "        item.text = clean_text(text)\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(xml_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "def read_and_search_keywords(xml_file, keywords):\n",
        "    \"\"\"\n",
        "    Read the XML file and search for the first occurrence of each keyword.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    results = {}\n",
        "    found_keywords = set()\n",
        "\n",
        "    text_lines = [elem.text or \"\" for elem in root.findall('Line')]\n",
        "\n",
        "    # Concatenate all texts into a single content block to mimic continuous reading\n",
        "    full_text = \" \".join(text_lines).lower()\n",
        "    keyword_positions = {kw.lower(): full_text.find(kw.lower()) for kw in keywords if full_text.find(kw.lower()) != -1}\n",
        "    # Sort keywords by their first occurrence position\n",
        "    sorted_keywords = sorted(keyword_positions.items(), key=lambda x: x[1])\n",
        "\n",
        "    for i, (kw, pos) in enumerate(sorted_keywords):\n",
        "        # Find end of the text slice for this keyword\n",
        "        if i < len(sorted_keywords) - 1:\n",
        "            end_pos = sorted_keywords[i + 1][1]\n",
        "        else:\n",
        "            end_pos = len(full_text)\n",
        "        # Extract the text segment associated with the keyword\n",
        "        segment = full_text[pos:end_pos].strip()\n",
        "        results[kw] = segment\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '2.pdf'\n",
        "    xml_path = os.path.splitext(pdf_path)[0] + '.xml'\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    all_lines = []\n",
        "    for image in images:\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB if needed\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "        # Perform OCR\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        all_lines.extend(lines)\n",
        "\n",
        "    # Write the OCR results to XML\n",
        "    write_to_xml(all_lines, xml_path)\n",
        "    print(f\"XML file has been created: {xml_path}\")\n",
        "\n",
        "    # Keywords to search in the XML\n",
        "    keywords = [\"Fastighetsagare\", \"Fastighetsbeteckning\", \"anlaggare\", \"ort, datum\", \"organisationsnumen\" \"Anlaggare av ledning\"]\n",
        "\n",
        "    # Read the XML and search for keywords\n",
        "    search_results = read_and_search_keywords(xml_path, keywords)\n",
        "\n",
        "    # Print the search results\n",
        "    for kw, content in search_results.items():\n",
        "        print(f\"{kw.capitalize()}: {content}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zj9fHgQJuWB",
        "outputId": "37cbb0a3-b530-4025-aaf0-16a9a29ab544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 39s 39s/step\n",
            "9/9 [==============================] - 63s 7s/step\n",
            "1/1 [==============================] - 41s 41s/step\n",
            "4/4 [==============================] - 27s 7s/step\n",
            "1/1 [==============================] - 38s 38s/step\n",
            "1/1 [==============================] - 0s 229ms/step\n",
            "XML file has been created: 2.xml\n",
            "Fastighetsagare: fastighetsagare rislidens byaforening norsjo 935 81 risliden\n",
            "Fastighetsbeteckning: fastighetsbeteckning 241\n",
            "Anlaggare: anlaggare nod norsj o kommun av storgatan 67 norsjo 935 81 organisationsnumen 2120002858 nyttjanderatten fastighetsagaren till lokalutrymme pa fastighet for installera anlaggaren ratt angiven att ger tillhorande bibehalla teknisk nodj for drift itanl aggning jamte och apparatur av kanalisation ledningsdragning laget redovisas pa bifogad och karta overgar nyttja anderatten automatiskt till den anlaggnings vid byte anlaggningsagare nye av agaren medfora forandring flyttning anlaggning den nar planerar arbete kan eller ska part som av god eller andring omfattning andra tid kontaktas l fraga flyttning storre parten 1 om av sarskild ffas arbetets samordning tidplan for skall overenskommelse trat och om genomforandet om fastighetsagaren overlater fastighet byggnad eller anl aggning beror annan som forhallande till forvarvaren anlaggarens enligt detta avtal ska fastighetsagaren gora ratt 1 forbehall for anlaggarens enligt detta avtal ratt overlatelse fastighet norsjo kommun for vid forsaljning eller ska kontakt tas med annan av upprattande kontrakt nytt av ar innan avtalet upphor forlangs avtalet giltigt 10 ar om avtalet inte sags l ar senast upp det pa ytterligare s ar 1 taget lokalnyttj ande mojlighet for tillhandahaller elektrisk stromkalla frostfritt och fastighetsagaren utrymme enligt bilagda foreskrifter anordna tilltradesskydd till anlaggningen anlaggaren att atgard kanalisationen och teknisk det aligger anlaggaren efter varje 1 att apparatur aterstalla eventuella skador eller anordning 1 utrymmet annan reparera underlatta mojligheter fastighetsagaren forbinder medverka till anlaggarens att sig att att tillhorande ledningar kanalisation anlaggaren fritt tilltrade till skota noden med och ager med tilltradesregler omsesidigt overenskommes fastigheten de som 2 2 pa egendom tillhorig fastighetsagaren anlaggaren for skada fast och orsakats ska som av uppgorelse traffas varje enskilt falls 1 eventuell flyttning nod ocheller kanalisation bekostas den ha part nytta av som anses av atgarden detta galler saval under gallande avtal avtalet nar sagts som av upps bort inte fastig upphor avtalet helt eller delvis anlaggningen anlaggaren och tas om nagon anlaggningsdel far lamnas hetsagaren overenskommer kvar att om vilka fastighetsagaren tagit exemplar och detta avtal har upprattats i exemplar ett tre av tva exemplar anlaggare tagit ider norso kis 51i deok il oi ausaulua ort datum datum douatsahous aey cles norsjio kommun fastighetsagare for bauustioul t leucu ake thorbjorn olsson eriksson namnfortydligande textal namnfortydligande itexta namnfortydligande textal bilagai karta 1 lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate CER, WER just for one PDF\n",
        "import os\n",
        "import keras_ocr\n",
        "from pdf2image import convert_from_path\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def preprocess_image(image, size=(1280, 720)):\n",
        "    \"\"\"\n",
        "    Preprocess the image by resizing, converting to grayscale, and applying Gaussian blur.\n",
        "    Then convert it back to RGB as keras-ocr expects a three-channel image.\n",
        "    \"\"\"\n",
        "    image = image.resize(size)\n",
        "    gray_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
        "    preprocessed_image = cv2.cvtColor(blurred_image, cv2.COLOR_GRAY2RGB)\n",
        "    return preprocessed_image\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Normalize the text by removing invalid XML characters and other non-alphanumeric characters.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    text = invalid_xml_chars.sub('', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def process_single_document(pdf_path, ground_truth_path):\n",
        "    # Setup the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    if not os.path.exists(ground_truth_path):\n",
        "        print(f\"Missing ground truth text file for {pdf_path}\")\n",
        "        return\n",
        "\n",
        "    images = convert_from_path(pdf_path)\n",
        "    all_text = []\n",
        "\n",
        "    for image in images:\n",
        "        preprocessed_image = preprocess_image(image)\n",
        "        prediction_groups = pipeline.recognize([np.array(preprocessed_image)])\n",
        "        for predictions in prediction_groups:\n",
        "            text = ' '.join([word[0] for word in predictions])\n",
        "            clean_text_data = clean_text(text)\n",
        "            all_text.append(clean_text_data)\n",
        "\n",
        "    ocr_output_text = '\\n'.join(all_text)\n",
        "\n",
        "    with open(ground_truth_path, 'r', encoding='utf-8') as gt_file:\n",
        "        ground_truth_text = gt_file.read()\n",
        "        ground_truth_text = clean_text(ground_truth_text)\n",
        "\n",
        "    calculated_wer = wer(ground_truth_text, ocr_output_text)\n",
        "    calculated_cer = cer(ground_truth_text, ocr_output_text)\n",
        "\n",
        "    print(f\"\\nOCR Text: {ocr_output_text}\")\n",
        "    print(f\"\\nGround Truth Text: {ground_truth_text}\")\n",
        "    print(f\"\\nWER: {calculated_wer:.2f}\")\n",
        "    print(f\"CER: {calculated_cer:.2f}\")\n",
        "\n",
        "# Example file paths\n",
        "pdf_path = '2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.pdf'\n",
        "ground_truth_path = '2EDEPZ4VHTLPTWSZR6FAVUJ3B2ZVSIPS.txt'\n",
        "process_single_document(pdf_path, ground_truth_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QikUmuTxOmWA",
        "outputId": "8e00fa96-bf91-4484-b11a-d7432ecd3ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 0s 461ms/step\n",
            "4/4 [==============================] - 2s 14ms/step\n",
            "\n",
            "OCR Text: adoes czaood ued raooalanes cranaaain starsee larate coaod codcooco oonae coa  cerons t loo soto rston alod ted uds eoole caaei sototans manoes e oroane srirazes rns oooed ceo rantine co orae cee lalees cottao caduo dlannas cooe cocas caooed conod cured cro nntsn oold lolns uoloed ood efeinn slasle udes somslons wtoles oanna only dy cored oaoes cadad ce to clacle nantoe csln dad soed tacoros talsantl srnaizan clod cood seloo acaoien coes aaatan ded en nes tansnes raasen scod uno et cioed on slagnss ce dadleed doted de coo oolans iadid de uanes olae onoatn cooslor coeed ood saiens od co\n",
            "\n",
            "Ground Truth Text: 6172009 US religious freedom watchdog barred from India  Agence France Presse\n",
            "\n",
            "   httpwwwgooglecomhostednewsafparticleALeqM5g6pjdmBNAEyFXqsYygocqBt5wQ  \n",
            "\n",
            "        WASHINGTON AFP  The US government watchdog on religious freedom abroad\n",
            "criticized India for refusing to grant its representatives visas after their planned trip came under\n",
            "fire from Hindu conservatives\n",
            "\n",
            "    India joins only Cuba in refusing a visit by the US commission which has been allowed to\n",
            "visit even nations whose records it frequently criticizes such as China and Saudi Arabia\n",
            "\n",
            "    A delegation of the US Commission on International Religious Freedom USCIRF had\n",
            "planned to leave on June 12 for India where it has voiced concern about a rise in communal\n",
            "violence\n",
            "\n",
            " 1  1\n",
            "\n",
            "       \n",
            " \n",
            "  \n",
            " \n",
            "    \n",
            "    \n",
            " \n",
            "\n",
            "\n",
            "WER: 1.02\n",
            "CER: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert pdf to txt file\n",
        "!pip install pdfminer.six\n",
        "import os\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "def convert_pdf_to_txt(pdf_path, txt_path):\n",
        "    \"\"\"\n",
        "    Convert a PDF file to a text file using pdfminer.six.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = extract_text(pdf_path)\n",
        "        with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "            txt_file.write(text)\n",
        "        print(f\"Converted {pdf_path} to {txt_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {pdf_path}: {str(e)}\")\n",
        "\n",
        "def convert_pdfs_in_directory(directory):\n",
        "    \"\"\"\n",
        "    Convert all PDF files to text files.\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(directory, filename)\n",
        "            txt_path = os.path.join(directory, filename.replace('.pdf', '.txt'))\n",
        "            convert_pdf_to_txt(pdf_path, txt_path)\n",
        "\n",
        "def main():\n",
        "    directory_path = 'text/'\n",
        "    convert_pdfs_in_directory(directory_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQXsUP0narWi",
        "outputId": "29d53701-2b5b-4240-ced5-91e604315392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Converted text/7MHCNBM2ZR4PD2BSWVFLG77BRN3HS3AD.pdf to text/7MHCNBM2ZR4PD2BSWVFLG77BRN3HS3AD.txt\n",
            "Converted text/27UIROOYZ4IE3FKKNAPCKEQXWBKKN7XM.pdf to text/27UIROOYZ4IE3FKKNAPCKEQXWBKKN7XM.txt\n",
            "Converted text/7LAHAWE6SGPL25FWUGMRKXCMTD3CWEBO.pdf to text/7LAHAWE6SGPL25FWUGMRKXCMTD3CWEBO.txt\n",
            "Converted text/7CIFOV7XZXS6EHUFYZF6H3NCN637PI2M.pdf to text/7CIFOV7XZXS6EHUFYZF6H3NCN637PI2M.txt\n",
            "Converted text/7GYQLFBVGRTYWE73NKCNCODTJVURFULB.pdf to text/7GYQLFBVGRTYWE73NKCNCODTJVURFULB.txt\n",
            "Converted text/73KIZM2KPVDJHMKWM5X36DBUKAOOCWZ4.pdf to text/73KIZM2KPVDJHMKWM5X36DBUKAOOCWZ4.txt\n",
            "Converted text/6C5QST2YJX7GA4BOHPZK66SWOSEATYGO.pdf to text/6C5QST2YJX7GA4BOHPZK66SWOSEATYGO.txt\n",
            "Converted text/6X3XULQL4IDEVAAER5F2M3RJGJS2REAH.pdf to text/6X3XULQL4IDEVAAER5F2M3RJGJS2REAH.txt\n",
            "Converted text/7PZU4ANBX7CFDAKIK25A2U6GR5OMGEZX.pdf to text/7PZU4ANBX7CFDAKIK25A2U6GR5OMGEZX.txt\n",
            "Converted text/35DKLURQRIKJHCGH3IP5TO2BTAOV4CBN.pdf to text/35DKLURQRIKJHCGH3IP5TO2BTAOV4CBN.txt\n",
            "Converted text/6GHFGNIZJZWEVRNG4J5ILE62VJ3PUURO.pdf to text/6GHFGNIZJZWEVRNG4J5ILE62VJ3PUURO.txt\n",
            "Converted text/6RZXET5EGN7QMGM5PROIXJI447YTCETZ.pdf to text/6RZXET5EGN7QMGM5PROIXJI447YTCETZ.txt\n",
            "Converted text/32PQ6TLRU26TDGA4NLYBMYEHYGX3X7US.pdf to text/32PQ6TLRU26TDGA4NLYBMYEHYGX3X7US.txt\n",
            "Converted text/6MPUNFSLSFKNLOGE2I6WRELWSJOF27UK.pdf to text/6MPUNFSLSFKNLOGE2I6WRELWSJOF27UK.txt\n",
            "Converted text/7YLE2QQTJK2HP5N2PUSAA3IKH662U3Y6.pdf to text/7YLE2QQTJK2HP5N2PUSAA3IKH662U3Y6.txt\n",
            "Converted text/7IYBO6EFOFMF4ETCCIZYJEGPTBQY3EE5.pdf to text/7IYBO6EFOFMF4ETCCIZYJEGPTBQY3EE5.txt\n",
            "Converted text/6Z3CUWVPRPVEBPROWQCD6EZJ22CLL5OY.pdf to text/6Z3CUWVPRPVEBPROWQCD6EZJ22CLL5OY.txt\n",
            "Converted text/7HBBXA2H7AQGVZSBZTLTPKAAEE3ZY2YK.pdf to text/7HBBXA2H7AQGVZSBZTLTPKAAEE3ZY2YK.txt\n",
            "Converted text/6OV4KF7OC77SI4QQ6W5MKPRKLFKJFMSA.pdf to text/6OV4KF7OC77SI4QQ6W5MKPRKLFKJFMSA.txt\n",
            "Converted text/35CQZCOMND26DXY46FVVS47WB2AL75OK.pdf to text/35CQZCOMND26DXY46FVVS47WB2AL75OK.txt\n",
            "Converted text/7RVVK7WHY6HBTX3RU6AVLEEPLNLNQQDZ.pdf to text/7RVVK7WHY6HBTX3RU6AVLEEPLNLNQQDZ.txt\n",
            "Converted text/6LM5MPDEKTK4FAVUBSU5HZOLO33XZOV6.pdf to text/6LM5MPDEKTK4FAVUBSU5HZOLO33XZOV6.txt\n",
            "Converted text/6AVXT3OLPGYVAG7L4CBYJHXLAXJE4V6F.pdf to text/6AVXT3OLPGYVAG7L4CBYJHXLAXJE4V6F.txt\n",
            "Converted text/7N6KRBZIEFV4F5QLLW3GBF6LKNNWSWVB.pdf to text/7N6KRBZIEFV4F5QLLW3GBF6LKNNWSWVB.txt\n",
            "Converted text/7DHC66NNE4RHBN7MNVPKSCN6E2GFRS2Z.pdf to text/7DHC66NNE4RHBN7MNVPKSCN6E2GFRS2Z.txt\n",
            "Converted text/7A3MBRLFC6OU5KGMFIDEQPUOQTROBYUS.pdf to text/7A3MBRLFC6OU5KGMFIDEQPUOQTROBYUS.txt\n",
            "Converted text/6A2V6MYYSW4C6QUKXJX7BEGNPYKZGV6O.pdf to text/6A2V6MYYSW4C6QUKXJX7BEGNPYKZGV6O.txt\n",
            "Converted text/6X7DGVAM2PXFTZ3B6YQOHRCZ3VVBS5LO.pdf to text/6X7DGVAM2PXFTZ3B6YQOHRCZ3VVBS5LO.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the time duration\n",
        "from pdf2image import convert_from_path\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import os\n",
        "import keras_ocr\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Remove invalid XML characters from the text.\n",
        "    \"\"\"\n",
        "    invalid_xml_chars = re.compile(u'[\\u0000-\\u0008\\u000B-\\u000C\\u000E-\\u001F]')\n",
        "    return invalid_xml_chars.sub('', text)\n",
        "\n",
        "def group_text_by_lines(prediction_groups):\n",
        "    \"\"\"\n",
        "    Group text by lines including their bounding boxes and confidences.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for word, box in prediction_groups[0]:\n",
        "        y_coords = [point[1] for point in box]\n",
        "        lines.append((word, box, 0.95))  # keras-ocr doesn't provide a confidence, so we use a dummy value\n",
        "    return lines\n",
        "\n",
        "def extract_specific_info_from_lines(lines, keywords, text_limits):\n",
        "    \"\"\"\n",
        "    Extracts specific information based on predefined keywords.\n",
        "    \"\"\"\n",
        "    pairs = {}\n",
        "    current_key = None\n",
        "    current_text = []\n",
        "    current_limit = None\n",
        "    normalized_keywords = {keyword.lower(): keyword for keyword in keywords}\n",
        "\n",
        "    for text, _, _ in lines:\n",
        "        normalized_text = text.lower().strip()\n",
        "        if any(normalized_text.startswith(keyword) for keyword in normalized_keywords):\n",
        "            if current_key:\n",
        "                formatted_text = ' '.join(current_text).strip()\n",
        "                if current_limit is not None:\n",
        "                    formatted_text = formatted_text[:current_limit]\n",
        "                pairs[current_key] = formatted_text\n",
        "            current_key = normalized_keywords.get(normalized_text, text.strip())\n",
        "            current_text = []\n",
        "            current_limit = text_limits.get(current_key, None)\n",
        "        elif current_key:\n",
        "            current_text.append(text.strip())\n",
        "\n",
        "    if current_key:\n",
        "        formatted_text = ' '.join(current_text).strip()\n",
        "        if current_limit is not None:\n",
        "            formatted_text = formatted_text[:current_limit]\n",
        "        pairs[current_key] = formatted_text\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize the keras-ocr pipeline\n",
        "    pipeline = keras_ocr.pipeline.Pipeline()\n",
        "\n",
        "    pdf_path = '4.pdf'\n",
        "    images = convert_from_path(pdf_path)\n",
        "    keywords = [\"Fastighetsagare\", \"Namn\", \"Postnr och ort\", \"Fastighetsbeteckning\", \"Anlaggare av ledning\"]\n",
        "    text_limits = {\"Anlaggare av ledning\": 41}\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        image_processing_start = time.time()  # Start timing the processing for each image\n",
        "\n",
        "        image_np = np.array(image)\n",
        "        if len(image_np.shape) == 2:  # Convert grayscale to RGB\n",
        "            image_np = np.stack([image_np]*3, axis=-1)\n",
        "\n",
        "        # Perform OCR\n",
        "        prediction_groups = pipeline.recognize([image_np])\n",
        "        lines = group_text_by_lines(prediction_groups)\n",
        "        info = extract_specific_info_from_lines(lines, keywords, text_limits)\n",
        "\n",
        "        for key in keywords:\n",
        "            value = info.get(key, \"\")\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "        image_processing_end = time.time()\n",
        "        print(f\"Processed image {i+1} in {image_processing_end - image_processing_start} seconds\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Total OCR processing time: {end_time - start_time} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmQg_cwpA46L",
        "outputId": "0c4d4cb4-32c3-4af9-b25d-65e22627dca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Downloading /root/.keras-ocr/craft_mlt_25k.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: resize_bilinear (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.image.resize(...method=ResizeMethod.BILINEAR...)` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "9/9 [==============================] - 4s 98ms/step\n",
            "Fastighetsagare: \n",
            "Namn: l arabere f adress nals so l s postnr och ort 412 nonsto\n",
            "Postnr och ort: \n",
            "Fastighetsbeteckning: anlaggare ledning norsjo kommun av storgatan 67 norsjo 935 81 2120002858 organisationsnr med underjordisk ledning for data och telekommunikation enligt detta avtal alla avses kablar och ledningar vilka overfors signaler for bild data eller i t genom ex annat underjordisk ledning ingar sadana andamal erforderliga for dess anordningar brunnar som skarvlador tillbehor tomror skap och andra\n",
            "Anlaggare av ledning: \n",
            "Processed image 1 in 16.055208921432495 seconds\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "7/7 [==============================] - 0s 72ms/step\n",
            "Fastighetsagare: \n",
            "Namn: \n",
            "Postnr och ort: \n",
            "Fastighetsbeteckning: \n",
            "Anlaggare av ledning: \n",
            "Processed image 2 in 1.6911015510559082 seconds\n",
            "Total OCR processing time: 24.213207960128784 seconds\n"
          ]
        }
      ]
    }
  ]
}